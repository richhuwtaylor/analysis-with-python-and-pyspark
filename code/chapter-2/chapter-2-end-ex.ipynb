{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    explode,\n",
    "    greatest,\n",
    "    length,\n",
    "    lower,\n",
    "    regexp_extract,\n",
    "    split\n",
    ")\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a SparkSession, giving it a relevant appName\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"End of Chapter 2 exercises.\"\n",
    ").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "\n",
    "Given the following data frame, programmatically count the number of columns that\n",
    "aren’t strings (answer = only one column isn’t a string).\n",
    "`createDataFrame()` allows you to create a data frame from a variety of sources,\n",
    "such as a pandas data frame or (in this case) a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo2_2_df = spark.createDataFrame([[\"test\", \"more test\", 10_000_000_000]], [\"one\", \"two\", \"three\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_type_columns(df, target_type=\"string\"):\n",
    "    # define the target type we (don't) want to count\n",
    "\n",
    "    # Get the columns of the target type\n",
    "    target_columns = [col for col, dtype in df.dtypes if dtype != target_type]\n",
    "\n",
    "    # Count the number of columns of the target type\n",
    "    num_columns = len(target_columns)\n",
    "\n",
    "    return num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-string columns: 1\n"
     ]
    }
   ],
   "source": [
    "target_type = \"string\"\n",
    "print(f\"Number of non-{target_type} columns: {non_type_columns(exo2_2_df, target_type)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "\n",
    "Rewrite the following code snippet, removing the `withColumnRenamed` method. Which\n",
    "version is clearer and easier to read?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `length` function returns the number of characters in a string column.\n",
    "exo2_3_df = (\n",
    " spark.read.text(\"../../data/gutenberg_books/1342-0.txt\")\n",
    " .select(length(col(\"value\")))\n",
    " .withColumnRenamed(\"length(value)\", \"number_of_char\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo2_3_df = (\n",
    " spark.read.text(\"../../data/gutenberg_books/1342-0.txt\")\n",
    " .select(length(col(\"value\")).alias(\"number_of_char\"))\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4\n",
    "Assume a data frame exo2_4_df. The following code block gives an error. What is the\n",
    "problem, and how can you solve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo2_4_df = spark.createDataFrame([[\"key\", 10_000, 20_000]], [\"key\", \"value1\", \"value2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `key` cannot be resolved. Did you mean one of the following? [`maximum_value`].;\n",
      "'Project ['key, 'max_value]\n",
      "+- Project [greatest(value1#43L, value2#44L) AS maximum_value#54L]\n",
      "   +- LogicalRDD [key#42, value1#43L, value2#44L], false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# `greatest` will return the greatest value of the list of column names,\n",
    "# skipping null value\n",
    "\n",
    "# The following statement will return an error\n",
    "try:\n",
    "    exo2_4_mod = exo2_4_df.select(\n",
    "        greatest(col(\"value1\"), col(\"value2\")).alias(\"maximum_value\")\n",
    "    ).select(\"key\", \"max_value\")\n",
    "except AnalysisException as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    exo2_4_mod = exo2_4_df.select(\n",
    "        col('key'),\n",
    "        greatest(col(\"value1\"), col(\"value2\")).alias(\"maximum_value\")\n",
    "    )\n",
    "except AnalysisException as err:\n",
    "    print(err)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.5\n",
    "\n",
    "Let’s take our words_nonull data frame, available in the next listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = spark.read.text(\"../../data/gutenberg_books/1342-0.txt\")\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))\n",
    "words_clean = words_lower.select(\n",
    " regexp_extract(col(\"word_lower\"), \"[a-z]*\", 0).alias(\"word\")\n",
    ")\n",
    "words_nonull = words_clean.where(col(\"word\") != \"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Remove all of the occurrences of the word is.\n",
    "\n",
    "b) (Challenge) Using the length function, keep only the words with more than three\n",
    "characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_without_is = words_nonull.filter(col(\"word\") != \"is\")\n",
    "\n",
    "words_filtered = words_nonull.filter(length(col(\"word\")) > 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.6\n",
    "\n",
    "The where clause takes a Boolean expression over one or many columns to filter the\n",
    "data frame. Beyond the usual Boolean operators (>, <, ==, <=, >=, !=), PySpark provides other functions that return Boolean columns in the pyspark.sql.functions\n",
    "module.\n",
    "\n",
    "A good example is the `isin()` method (applied on a Column object, like\n",
    "`col(…).isin(…))`, which takes a list of values as a parameter, and will return only the\n",
    "records where the value in the column equals a member of the list.`\n",
    "\n",
    " Let’s say you want to remove the words is, not, the and if from your list of words,\n",
    "using a single `where()` method on the `words_nonull` data frame. Write the code to\n",
    "do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_without_is_not_the_if = words_nonull.where(~col(\"word\").isin([\"is\", \"not\", \"the\", \"if\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.7\n",
    "\n",
    "One of your friends comes to you with the following code. They have no idea why it\n",
    "doesn’t work. Can you diagnose the problem in the try block, explain why it is an\n",
    "error, and provide a fix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m     book \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mtext(\u001b[39m\"\u001b[39m\u001b[39m../../data/gutenberg_books/1342-0.txt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m     book \u001b[39m=\u001b[39m book\u001b[39m.\u001b[39mprintSchema()\n\u001b[1;32m----> 4\u001b[0m     lines \u001b[39m=\u001b[39m book\u001b[39m.\u001b[39;49mselect(split(book\u001b[39m.\u001b[39mvalue, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mline\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m      5\u001b[0m     words \u001b[39m=\u001b[39m lines\u001b[39m.\u001b[39mselect(explode(col(\u001b[39m\"\u001b[39m\u001b[39mline\u001b[39m\u001b[39m\"\u001b[39m))\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mword\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[39mexcept\u001b[39;00m AnalysisException \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    book = spark.read.text(\"../../data/gutenberg_books/1342-0.txt\")\n",
    "    book = book.printSchema()\n",
    "    lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "    words = lines.select(explode(col(\"line\")).alias(\"word\"))\n",
    "except AnalysisException as err:\n",
    "    print(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book = spark.read.text(\"../../data/gutenberg_books/1342-0.txt\")\n",
    "book.printSchema()\n",
    "lines = book.select(split(book.value, \" \").alias(\"line\"))\n",
    "words = lines.select(explode(col(\"line\")).alias(\"word\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
