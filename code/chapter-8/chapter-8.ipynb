{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fractions import Fraction\n",
    "from typing import Tuple, Optional\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from py4j.protocol import Py4JJavaError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# RDD  functions and methods are under the SparkContext object,\n",
    "# which is an attribute of SparkSession. We alias it here for convenience.\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collection of multiple unrelated types\n",
    "collection = [1, \"two\", 3.0, (\"four\", 4), {\"five\": 5}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promote the list to an RDD using parallelize method\n",
    "collection_rdd = sc.parallelize(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:287\n"
     ]
    }
   ],
   "source": [
    "print(collection_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping a simple function, add_one(), to each element\n",
    "\n",
    "def add_one(value):\n",
    "    return value + 1\n",
    "\n",
    "collection_rdd = collection_rdd.map(add_one)\n",
    "\n",
    "try:\n",
    "    # collect() materialises an RDD into a Python list on the master node\n",
    "    print(collection_rdd.collect())\n",
    "except Py4JJavaError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 'two', 4.0, ('four', 4), {'five': 5}]\n"
     ]
    }
   ],
   "source": [
    "# Mapping a safer_add_one() to each element in an RDD\n",
    "\n",
    "collection_rdd = sc.parallelize(collection)\n",
    "\n",
    "def safer_add_one(value):\n",
    "    try:\n",
    "        return value + 1\n",
    "    except TypeError:\n",
    "        # Return the original value untouched if encounter TypeError\n",
    "        return value\n",
    "    \n",
    "collection_rdd = collection_rdd.map(safer_add_one)\n",
    "\n",
    "print(collection_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4.0]\n"
     ]
    }
   ],
   "source": [
    "# Filtering an RDD with a lambda function\n",
    "collection_rdd = collection_rdd.filter(\n",
    "    lambda elem: isinstance(elem, (float, int))\n",
    ")\n",
    "\n",
    "print(collection_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "# Applying the add() funciton via reduce()\n",
    "collection_rdd = sc.parallelize([4, 7, 9, 1, 3])\n",
    "\n",
    "print(collection_rdd.reduce(add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.1\n",
    "\n",
    "The PySpark RDD API provides a `count()` method that returns the number of elements in the RDD as an integer. Reproduce the behavior of this method using `map()`, `filter()`, and/or `reduce()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "count = collection_rdd.map(lambda x: 1).reduce(lambda x, y: x + y)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.3\n",
    "\n",
    "What is the return value of the following code block?\n",
    "\n",
    "```\n",
    "a_rdd = sc.parallelize([0, 1, None, [], 0.0])\n",
    "a_rdd.filter(lambda x: x).collect()\n",
    "```\n",
    "\n",
    "Answer: `[1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|fraction|\n",
      "+--------+\n",
      "|[0, 1]  |\n",
      "|[0, 2]  |\n",
      "|[0, 3]  |\n",
      "|[0, 4]  |\n",
      "|[0, 5]  |\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a data frame containing a single-array column\n",
    "fractions = [[x, y] for x in range(100) for y in range(1, 100)]\n",
    "\n",
    "frac_df = spark.createDataFrame(fractions, [\"numerator\", \"denominator\"])\n",
    "\n",
    "# array() takes two or more columns of the same type and creates a single column\n",
    "# containing an array of the columns passed as a parameter\n",
    "frac_df = frac_df.select(\n",
    "    F.array(F.col(\"numerator\"), F.col(\"denominator\")).alias(\"fraction\")\n",
    ")\n",
    "\n",
    "frac_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type synonym: \"When you see Frac, assume it's a Tuple[int, int]\"\n",
    "Frac = Tuple[int, int]\n",
    "\n",
    "def py_reduce_fraction(frac: Frac) -> Optional[Frac]:\n",
    "    \"\"\"Reduce a fraction represented as a 2-tuple of integers\"\"\"\n",
    "    num, denom = frac\n",
    "    if denom:\n",
    "        answer = Fraction(num, denom)\n",
    "        return answer.numerator, answer.denominator\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert py_reduce_fraction((3, 6)) == (1, 2) \n",
    "assert py_reduce_fraction((1, 0)) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def py_fraction_to_float(frac: Frac) -> Optional[float]:\n",
    "    \"\"\"Transforms a fraction represented as a 2-tuple of integers into a float.\"\"\"\n",
    "    num, denom = frac\n",
    "    if denom:\n",
    "        return num / denom\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert py_fraction_to_float((2, 8)) == 0.25\n",
    "assert py_fraction_to_float((10, 0)) is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|fraction|reduced_fraction|\n",
      "+--------+----------------+\n",
      "|[0, 1]  |[0, 1]          |\n",
      "|[0, 2]  |[0, 1]          |\n",
      "|[0, 3]  |[0, 1]          |\n",
      "|[0, 4]  |[0, 1]          |\n",
      "|[0, 5]  |[0, 1]          |\n",
      "+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a UDF explicitly with the udf() function\n",
    "\n",
    "# Alias an array of long PySpark type to SparkFrac\n",
    "SparkFrac = T.ArrayType(T.LongType())\n",
    "\n",
    "# Promote the Python function using the udf() function\n",
    "reduce_fraction = F.udf(py_reduce_fraction, SparkFrac)\n",
    "\n",
    "frac_df = frac_df.withColumn(\n",
    "    \"reduced_fraction\", reduce_fraction(F.col(\"fraction\"))\n",
    ")\n",
    "\n",
    "frac_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a UDF directly using the udf() decorator\n",
    "\n",
    "# The decorator performs the same function as the udf() function,\n",
    "# but returns a UDF bearing the name of the function defined under it\n",
    "@F.udf(T.DoubleType())\n",
    "def fraction_to_float(frac: Frac) -> Optional[float]:\n",
    "    \"\"\"Transforms a fraction represented as a 2-tuple of integers into a float.\"\"\"\n",
    "    num, denom = frac\n",
    "    if denom:\n",
    "        return num / denom\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|reduced_fraction|fraction_float     |\n",
      "+----------------+-------------------+\n",
      "|[3, 50]         |0.06               |\n",
      "|[3, 67]         |0.04477611940298507|\n",
      "|[7, 76]         |0.09210526315789473|\n",
      "|[9, 23]         |0.391304347826087  |\n",
      "|[9, 25]         |0.36               |\n",
      "+----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "frac_df = frac_df.withColumn(\n",
    "    \"fraction_float\", fraction_to_float(F.col(\"reduced_fraction\"))\n",
    ")\n",
    "\n",
    "frac_df.select(\"reduced_fraction\", \"fraction_float\").distinct().show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert fraction_to_float.func((1, 2)) == 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.3\n",
    "\n",
    "Using the following definitions, create a temp_to_temp(value, from, to) that takes a\n",
    "numerical value in from degrees and converts it to degrees.\n",
    "\n",
    "- C = (F - 32) * 5 / 9 (Celcius)\n",
    "- K = C + 273.15 (Kelvin)\n",
    "- R = F + 459.67 (Rankine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_to_temp(value: float, from_unit: str, to_unit: str) -> float:\n",
    "    conversion_formulas = {\n",
    "        ('F', 'C'): lambda x: (x - 32) * 5 / 9,\n",
    "        ('F', 'K'): lambda x: (x + 459.67) * 5 / 9,\n",
    "        ('F', 'R'): lambda x: x + 459.67,\n",
    "        ('C', 'F'): lambda x: x * 9 / 5 + 32,\n",
    "        ('C', 'K'): lambda x: x + 273.15,\n",
    "        ('C', 'R'): lambda x: (x + 273.15) * 9 / 5,\n",
    "        ('K', 'F'): lambda x: x * 9 / 5 - 459.67,\n",
    "        ('K', 'C'): lambda x: x - 273.15,\n",
    "        ('K', 'R'): lambda x: x * 9 / 5,\n",
    "        ('R', 'F'): lambda x: x - 459.67,\n",
    "        ('R', 'C'): lambda x: (x - 459.67) * 5 / 9,\n",
    "        ('R', 'K'): lambda x: x * 5 / 9,\n",
    "    }\n",
    "\n",
    "    if from_unit not in conversion_formulas or to_unit not in conversion_formulas:\n",
    "        raise ValueError(\"Invalid temperature units. Supported units: 'F', 'C', 'K', 'R'\")\n",
    "\n",
    "    return conversion_formulas[(from_unit, to_unit)](value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.4\n",
    "\n",
    "Correct the following UDF, so it doesn’t generate an error.\n",
    "\n",
    "```\n",
    "@F.udf(T.IntegerType())\n",
    "def naive_udf(t: str) -> str:\n",
    "    return answer * 3.14159\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(T.IntegerType())\n",
    "def naive_udf(t: int) -> float:\n",
    "    return t * 3.14159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.28318"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_udf.func(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.5\n",
    "\n",
    "Create a UDF that adds two fractions together, and test it by adding the reduced_\n",
    "fraction to itself in the test_frac data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(SparkFrac)\n",
    "def add_two_factions(frac_a: Frac, frac_b: Frac)  -> Optional[Frac]:\n",
    "    \"\"\"Add two fractions together, represented as a 2-tuple of integers\"\"\"\n",
    "    num_a, denom_a = frac_a\n",
    "    num_b, denom_b = frac_b\n",
    "    if denom_a and denom_b:\n",
    "        if denom_a == denom_b:\n",
    "            return py_reduce_fraction((num_a + num_b, denom_a))\n",
    "        else:\n",
    "            return py_reduce_fraction((num_a*denom_b + num_b*denom_a, denom_a*denom_b))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_df = frac_df.withColumn(\n",
    "    \"fraction_sum\", add_two_factions(F.col(\"reduced_fraction\"), F.col(\"reduced_fraction\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.6\n",
    "\n",
    "Because of the `LongType()`, the `py_reduce_fraction` (see the previous exercise) will\n",
    "not work if the numerator or denominator exceeds `pow(2, 63)-1` or is lower than\n",
    "`-pow(2, 63).` Modify the `py_reduce_fraction` to return None if this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def py_reduce_fraction_mod(frac: Frac) -> Optional[Frac]:\n",
    "    \"\"\"Reduce a fraction represented as a 2-tuple of integers\"\"\"\n",
    "    num, denom = frac\n",
    "\n",
    "    # Check if the numerator or denominator exceeds the supported range\n",
    "    if not (-pow(2, 63) <= num < pow(2, 63)) or not (-pow(2, 63) <= denom < pow(2, 63)):\n",
    "        return None\n",
    "\n",
    "    if denom:\n",
    "        answer = Fraction(num, denom)\n",
    "        return answer.numerator, answer.denominator\n",
    "    return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
