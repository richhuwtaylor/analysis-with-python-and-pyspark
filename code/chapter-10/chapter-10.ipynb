{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from time import sleep\n",
    "from typing import Iterator, Sequence, Tuple\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we continue to work with the NOAA weather data we worked with in Chapter 9: 10 years' (2010 to 2020) worth of NOAA weather data located in Google BigQuery, which totals over 40 million records. The `bigquery-public-data` is a project available to all.\n",
    "\n",
    "Here, we read in a large amount of data from a warehouse and assemble a single data frame representing weather information across the globe for a period of 10 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Spark connector package version closest to our installed version of Spark (3.4.1)\n",
    "spark = SparkSession.builder.config(\n",
    "    \"spark.jars.packages\",\n",
    "    \"com.google.cloud.spark:spark-3.3-bigquery:0.32.0\"\n",
    ").config(\n",
    "    \"parentProject\", \"cool-wharf-393713\"\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract the table reading routine into a reusable function, returning the resulting data frame\n",
    "def read_df_from_bq(year):\n",
    "    return (\n",
    "        spark.read.format(\"bigquery\").option(\n",
    "            \"table\", f\"bigquery-public-data.noaa_gsod.gsod{year}\"\n",
    "        )\n",
    "        .option(\"credentialsFile\", \"../../../../cool-wharf-393713-73800a184f10.json\")\n",
    "        .load()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsod = (\n",
    "#     reduce(\n",
    "#         # use a lambda function over a list comprehension of data frames to union them all\n",
    "#         lambda x, y: x.unionByName(y, allowMissingColumns=True),\n",
    "#         [read_df_from_bq(year) for year in range(2010, 2021)],\n",
    "#     )\n",
    "#     .dropna(subset=[\"year\", \"mo\", \"da\", \"temp\"])\n",
    "#     .where(F.col(\"temp\") != 9999.9)\n",
    "#     .drop(\"date\")\n",
    "# )\n",
    "\n",
    "# Instead, we work locally with three year's worth of data in Parquet format:\n",
    "gsod = spark.read.parquet(\"../../data/window/gsod.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the coldest day of each year, the long way\n",
    "\n",
    "we want a data frame containing three records, one for each year and showing the station, the date (year, month, day), and the temperature of the coldest day recorded for that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|year|  temp|\n",
      "+----+------+\n",
      "|2017|-114.7|\n",
      "|2019|-114.7|\n",
      "|2018|-113.5|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the lowest temperature for each year using groupBy()\n",
    "coldest_temp = gsod.groupby(\"year\").agg(F.min(\"temp\").alias(\"temp\"))\n",
    "coldest_temp.orderBy(\"temp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+------+\n",
      "|   stn|year| mo| da|  temp|\n",
      "+------+----+---+---+------+\n",
      "|896250|2017| 06| 20|-114.7|\n",
      "|896060|2018| 08| 27|-113.5|\n",
      "|895770|2019| 06| 15|-114.7|\n",
      "+------+----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use a left-semi equi-join on the original table to get \n",
    "# the month and day columns\n",
    "coldest_when = gsod.join(\n",
    "    coldest_temp, how=\"left_semi\", on=[\"year\", \"temp\"]\n",
    ").select(\"stn\", \"year\", \"mo\", \"da\", \"temp\")\n",
    "\n",
    "coldest_when.orderBy(\"year\", \"mo\", \"da\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this instead using a window function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.window.WindowSpec object at 0x000001AE0FBAA040>\n"
     ]
    }
   ],
   "source": [
    "# Create a WindowSpec object by using the Window builder class\n",
    "# This forms a blueprint for an eventual window function.\n",
    "each_year = Window.partitionBy(\"year\")\n",
    "\n",
    "print(each_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+------+------+\n",
      "|year| mo| da|   stn|  temp|\n",
      "+----+---+---+------+------+\n",
      "|2017| 06| 20|896250|-114.7|\n",
      "|2018| 08| 27|896060|-113.5|\n",
      "|2019| 06| 15|895770|-114.7|\n",
      "+----+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the minimum temperature for each year using a window function\n",
    "(\n",
    "    gsod\n",
    "    .withColumn(\"min_temp\", F.min(\"temp\").over(each_year))\n",
    "    .where(\"temp = min_temp\")\n",
    "    .select(\"year\", \"mo\", \"da\", \"stn\", \"temp\")\n",
    "    .orderBy(\"year\", \"mo\", \"da\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+\n",
      "|   stn|year| mo| da|temp|count_temp|\n",
      "+------+----+---+---+----+----------+\n",
      "|994979|2017| 12| 11|21.3|        21|\n",
      "|998012|2017| 03| 02|31.4|        24|\n",
      "|719200|2017| 10| 09|60.5|        11|\n",
      "|917350|2018| 04| 21|82.6|         9|\n",
      "|076470|2018| 06| 07|65.0|        24|\n",
      "|996470|2018| 03| 12|55.6|        12|\n",
      "|041680|2019| 02| 19|16.1|        15|\n",
      "|949110|2019| 11| 23|54.9|        14|\n",
      "|998252|2019| 04| 18|44.7|        11|\n",
      "|998166|2019| 03| 20|34.8|        12|\n",
      "+------+----+---+---+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read gsod_light, a smaller data frame with 10 records\n",
    "# (so that we can see it in its entirety when show()ing it)\n",
    "gsod_light = spark.read.parquet(\"../../data/window/gsod_light.parquet\")\n",
    "gsod_light.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a window which partitions the data frame by month,\n",
    "# order each partition by the count_temp column\n",
    "temp_per_month_asc = Window.partitionBy(\"mo\").orderBy(\"count_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+--------+\n",
      "|   stn|year| mo| da|temp|count_temp|rank_tpm|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "|041680|2019| 02| 19|16.1|        15|       1|\n",
      "|996470|2018| 03| 12|55.6|        12|       1|\n",
      "|998166|2019| 03| 20|34.8|        12|       1|\n",
      "|998012|2017| 03| 02|31.4|        24|       3|\n",
      "|917350|2018| 04| 21|82.6|         9|       1|\n",
      "|998252|2019| 04| 18|44.7|        11|       2|\n",
      "|076470|2018| 06| 07|65.0|        24|       1|\n",
      "|719200|2017| 10| 09|60.5|        11|       1|\n",
      "|949110|2019| 11| 23|54.9|        14|       1|\n",
      "|994979|2017| 12| 11|21.3|        21|       1|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rank each day within its month account to count_temp column\n",
    "gsod_light.withColumn(\n",
    "    \"rank_tpm\", F.rank().over(temp_per_month_asc)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+--------+\n",
      "|   stn|year| mo| da|temp|count_temp|rank_tpm|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "|041680|2019| 02| 19|16.1|        15|       1|\n",
      "|996470|2018| 03| 12|55.6|        12|       1|\n",
      "|998166|2019| 03| 20|34.8|        12|       1|\n",
      "|998012|2017| 03| 02|31.4|        24|       2|\n",
      "|917350|2018| 04| 21|82.6|         9|       1|\n",
      "|998252|2019| 04| 18|44.7|        11|       2|\n",
      "|076470|2018| 06| 07|65.0|        24|       1|\n",
      "|719200|2017| 10| 09|60.5|        11|       1|\n",
      "|949110|2019| 11| 23|54.9|        14|       1|\n",
      "|994979|2017| 12| 11|21.3|        21|       1|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can avoid gaps in the ranking using dense_rank()\n",
    "gsod_light.withColumn(\n",
    "    \"rank_tpm\", F.dense_rank().over(temp_per_month_asc)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+------------------+\n",
      "|   stn|year| mo| da|temp|count_temp|          rank_tpm|\n",
      "+------+----+---+---+----+----------+------------------+\n",
      "|994979|2017| 12| 11|21.3|        21|               0.0|\n",
      "|998012|2017| 03| 02|31.4|        24|               0.5|\n",
      "|719200|2017| 10| 09|60.5|        11|               1.0|\n",
      "|996470|2018| 03| 12|55.6|        12|               0.0|\n",
      "|076470|2018| 06| 07|65.0|        24|               0.5|\n",
      "|917350|2018| 04| 21|82.6|         9|               1.0|\n",
      "|041680|2019| 02| 19|16.1|        15|               0.0|\n",
      "|998166|2019| 03| 20|34.8|        12|0.3333333333333333|\n",
      "|998252|2019| 04| 18|44.7|        11|0.6666666666666666|\n",
      "|949110|2019| 11| 23|54.9|        14|               1.0|\n",
      "+------+----+---+---+----+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute percentage rank for every recorded temperature per year\n",
    "temp_each_year = each_year.orderBy(\"temp\")\n",
    "\n",
    "gsod_light.withColumn(\n",
    "    \"rank_tpm\", F.percent_rank().over(temp_each_year)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+--------+\n",
      "|   stn|year| mo| da|temp|count_temp|rank_tpm|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "|994979|2017| 12| 11|21.3|        21|       1|\n",
      "|998012|2017| 03| 02|31.4|        24|       1|\n",
      "|719200|2017| 10| 09|60.5|        11|       2|\n",
      "|996470|2018| 03| 12|55.6|        12|       1|\n",
      "|076470|2018| 06| 07|65.0|        24|       1|\n",
      "|917350|2018| 04| 21|82.6|         9|       2|\n",
      "|041680|2019| 02| 19|16.1|        15|       1|\n",
      "|998166|2019| 03| 20|34.8|        12|       1|\n",
      "|998252|2019| 04| 18|44.7|        11|       2|\n",
      "|949110|2019| 11| 23|54.9|        14|       2|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the two-tile value over the window.\n",
    "# If a value overlaps with two tiles, it takes the value of the lowest one.\n",
    "gsod_light.withColumn(\"rank_tpm\", F.ntile(2).over(temp_each_year)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+--------+\n",
      "|   stn|year| mo| da|temp|count_temp|rank_tpm|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "|994979|2017| 12| 11|21.3|        21|       1|\n",
      "|998012|2017| 03| 02|31.4|        24|       2|\n",
      "|719200|2017| 10| 09|60.5|        11|       3|\n",
      "|996470|2018| 03| 12|55.6|        12|       1|\n",
      "|076470|2018| 06| 07|65.0|        24|       2|\n",
      "|917350|2018| 04| 21|82.6|         9|       3|\n",
      "|041680|2019| 02| 19|16.1|        15|       1|\n",
      "|998166|2019| 03| 20|34.8|        12|       2|\n",
      "|998252|2019| 04| 18|44.7|        11|       3|\n",
      "|949110|2019| 11| 23|54.9|        14|       4|\n",
      "+------+----+---+---+----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number records within each window partition using row_number()\n",
    "gsod_light.withColumn(\n",
    "    \"rank_tpm\", F.row_number().over(temp_each_year)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+----------+\n",
      "|   stn|year| mo| da|temp|count_temp|row_number|\n",
      "+------+----+---+---+----+----------+----------+\n",
      "|041680|2019| 02| 19|16.1|        15|         1|\n",
      "|998012|2017| 03| 02|31.4|        24|         1|\n",
      "|996470|2018| 03| 12|55.6|        12|         2|\n",
      "|998166|2019| 03| 20|34.8|        12|         3|\n",
      "|998252|2019| 04| 18|44.7|        11|         1|\n",
      "|917350|2018| 04| 21|82.6|         9|         2|\n",
      "|076470|2018| 06| 07|65.0|        24|         1|\n",
      "|719200|2017| 10| 09|60.5|        11|         1|\n",
      "|949110|2019| 11| 23|54.9|        14|         1|\n",
      "|994979|2017| 12| 11|21.3|        21|         1|\n",
      "+------+----+---+---+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a window with a descending-ordered column\n",
    "temp_per_month_desc = Window.partitionBy(\"mo\").orderBy(\n",
    "    F.col(\"count_temp\").desc()\n",
    ")\n",
    "\n",
    "gsod_light.withColumn(\n",
    "    \"row_number\", F.row_number().over(temp_per_month_desc)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytic functions: looking back and ahead\n",
    "\n",
    "We can access records bofore or after using LAG() and LEAD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+-------------+---------------+\n",
      "|   stn|year| mo| da|temp|count_temp|previous_temp|previous_temp_2|\n",
      "+------+----+---+---+----+----------+-------------+---------------+\n",
      "|994979|2017| 12| 11|21.3|        21|         null|           null|\n",
      "|998012|2017| 03| 02|31.4|        24|         21.3|           null|\n",
      "|719200|2017| 10| 09|60.5|        11|         31.4|           21.3|\n",
      "|996470|2018| 03| 12|55.6|        12|         null|           null|\n",
      "|076470|2018| 06| 07|65.0|        24|         55.6|           null|\n",
      "|917350|2018| 04| 21|82.6|         9|         65.0|           55.6|\n",
      "|041680|2019| 02| 19|16.1|        15|         null|           null|\n",
      "|998166|2019| 03| 20|34.8|        12|         16.1|           null|\n",
      "|998252|2019| 04| 18|44.7|        11|         34.8|           16.1|\n",
      "|949110|2019| 11| 23|54.9|        14|         44.7|           34.8|\n",
      "+------+----+---+---+----+----------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsod_light.withColumn(\n",
    "    \"previous_temp\", F.lag(\"temp\").over(temp_each_year)\n",
    ").withColumn(\n",
    "    \"previous_temp_2\", F.lag(\"temp\", 2).over(temp_each_year)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+------------------+------------------+\n",
      "|   stn|year| mo| da|temp|count_temp|      percent_rank|         cume_dist|\n",
      "+------+----+---+---+----+----------+------------------+------------------+\n",
      "|994979|2017| 12| 11|21.3|        21|               0.0|0.3333333333333333|\n",
      "|998012|2017| 03| 02|31.4|        24|               0.5|0.6666666666666666|\n",
      "|719200|2017| 10| 09|60.5|        11|               1.0|               1.0|\n",
      "|996470|2018| 03| 12|55.6|        12|               0.0|0.3333333333333333|\n",
      "|076470|2018| 06| 07|65.0|        24|               0.5|0.6666666666666666|\n",
      "|917350|2018| 04| 21|82.6|         9|               1.0|               1.0|\n",
      "|041680|2019| 02| 19|16.1|        15|               0.0|              0.25|\n",
      "|998166|2019| 03| 20|34.8|        12|0.3333333333333333|               0.5|\n",
      "|998252|2019| 04| 18|44.7|        11|0.6666666666666666|              0.75|\n",
      "|949110|2019| 11| 23|54.9|        14|               1.0|               1.0|\n",
      "+------+----+---+---+----+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare percent_rank() and cum_dist over a window\n",
    "gsod_light.withColumn(\n",
    "    \"percent_rank\", F.percent_rank().over(temp_each_year)\n",
    ").withColumn(\"cume_dist\", F.cume_dist().over(temp_each_year)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.2\n",
    "\n",
    "If you have a window where all the ordered values are the same, what is the result of applying ntile() to the window?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|index|value|\n",
      "+-----+-----+\n",
      "|    0|    2|\n",
      "|    0|    2|\n",
      "|    0|    2|\n",
      "|    0|    2|\n",
      "|    1|    2|\n",
      "|    1|    2|\n",
      "|    1|    2|\n",
      "|    1|    2|\n",
      "|    2|    2|\n",
      "|    2|    2|\n",
      "|    2|    2|\n",
      "|    2|    2|\n",
      "|    3|    2|\n",
      "|    3|    2|\n",
      "|    3|    2|\n",
      "|    3|    2|\n",
      "|    4|    2|\n",
      "|    4|    2|\n",
      "|    4|    2|\n",
      "|    4|    2|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ntile_example = spark.createDataFrame(\n",
    "    [[x // 4, 2] for x in range(1001)], [\"index\", \"value\"] \n",
    ")\n",
    "\n",
    "ntile_example.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.window.WindowSpec object at 0x000001AE152C15E0>\n"
     ]
    }
   ],
   "source": [
    "ntile_window = Window.partitionBy(\"index\").orderBy(\"value\")\n",
    "\n",
    "print(ntile_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|index|value|3tile|\n",
      "+-----+-----+-----+\n",
      "|    0|    2|    1|\n",
      "|    0|    2|    1|\n",
      "|    0|    2|    2|\n",
      "|    0|    2|    3|\n",
      "|    1|    2|    1|\n",
      "|    1|    2|    1|\n",
      "|    1|    2|    2|\n",
      "|    1|    2|    3|\n",
      "|    2|    2|    1|\n",
      "|    2|    2|    1|\n",
      "+-----+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ntile_example.withColumn(\"3tile\", F.ntile(3).over(ntile_window)).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using row and range boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+------------------+------------------+\n",
      "|   stn|year| mo| da|temp|count_temp|            avg_NO|             avg_O|\n",
      "+------+----+---+---+----+----------+------------------+------------------+\n",
      "|994979|2017| 12| 11|21.3|        21|37.733333333333334|              21.3|\n",
      "|998012|2017| 03| 02|31.4|        24|37.733333333333334|             26.35|\n",
      "|719200|2017| 10| 09|60.5|        11|37.733333333333334|37.733333333333334|\n",
      "|996470|2018| 03| 12|55.6|        12| 67.73333333333333|              55.6|\n",
      "|076470|2018| 06| 07|65.0|        24| 67.73333333333333|              60.3|\n",
      "|917350|2018| 04| 21|82.6|         9| 67.73333333333333| 67.73333333333333|\n",
      "|041680|2019| 02| 19|16.1|        15|            37.625|              16.1|\n",
      "|998166|2019| 03| 20|34.8|        12|            37.625|             25.45|\n",
      "|998252|2019| 04| 18|44.7|        11|            37.625|31.866666666666664|\n",
      "|949110|2019| 11| 23|54.9|        14|            37.625|            37.625|\n",
      "+------+----+---+---+----+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We get different results when we average over an ordered window\n",
    "# than we do when we average over an unordered window\n",
    "not_ordered = Window.partitionBy(\"year\")\n",
    "ordered = not_ordered.orderBy(\"temp\")\n",
    "\n",
    "gsod_light.withColumn(\n",
    "    \"avg_NO\", F.avg(\"temp\").over(not_ordered)\n",
    ").withColumn(\n",
    "    \"avg_O\", F.avg(\"temp\").over(ordered)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a window spec with explicit window boundaries\n",
    "not_ordered = Window.partitionBy(\"year\").rowsBetween(\n",
    "    Window.unboundedPreceding, Window.unboundedFollowing\n",
    ")\n",
    "ordered = not_ordered.partitionBy(\"temp\").rowsBetween(\n",
    "    Window.unboundedPreceding, Window.currentRow\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+----------+----------+\n",
      "|   stn|year| mo| da|temp|count_temp|        dt|    dt_num|\n",
      "+------+----+---+---+----+----------+----------+----------+\n",
      "|994979|2019| 12| 11|21.3|        21|2019-12-11|1576022400|\n",
      "|998012|2019| 03| 02|31.4|        24|2019-03-02|1551484800|\n",
      "|719200|2019| 10| 09|60.5|        11|2019-10-09|1570575600|\n",
      "|917350|2019| 04| 21|82.6|         9|2019-04-21|1555801200|\n",
      "|076470|2019| 06| 07|65.0|        24|2019-06-07|1559862000|\n",
      "|996470|2019| 03| 12|55.6|        12|2019-03-12|1552348800|\n",
      "|041680|2019| 02| 19|16.1|        15|2019-02-19|1550534400|\n",
      "|949110|2019| 11| 23|54.9|        14|2019-11-23|1574467200|\n",
      "|998252|2019| 04| 18|44.7|        11|2019-04-18|1555542000|\n",
      "|998166|2019| 03| 20|34.8|        12|2019-03-20|1553040000|\n",
      "+------+----+---+---+----+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a date column to apply range a range window on\n",
    "gsod_light_p = (\n",
    "    gsod_light\n",
    "    .withColumn(\"year\", F.lit(2019))\n",
    "    .withColumn(\"dt\",\n",
    "             F.to_date(\n",
    "                 F.concat_ws(\"-\", F.col(\"year\"), F.col(\"mo\"), F.col(\"da\"))\n",
    "             )\n",
    "        )\n",
    "    .withColumn(\"dt_num\", F.unix_timestamp(\"dt\"))\n",
    ")\n",
    "\n",
    "gsod_light_p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+----+----------+----------+----------+------------------+\n",
      "|   stn|year| mo| da|temp|count_temp|        dt|    dt_num|         avg_count|\n",
      "+------+----+---+---+----+----------+----------+----------+------------------+\n",
      "|041680|2019| 02| 19|16.1|        15|2019-02-19|1550534400|             15.75|\n",
      "|998012|2019| 03| 02|31.4|        24|2019-03-02|1551484800|             15.75|\n",
      "|996470|2019| 03| 12|55.6|        12|2019-03-12|1552348800|             15.75|\n",
      "|998166|2019| 03| 20|34.8|        12|2019-03-20|1553040000|              14.8|\n",
      "|998252|2019| 04| 18|44.7|        11|2019-04-18|1555542000|10.666666666666666|\n",
      "|917350|2019| 04| 21|82.6|         9|2019-04-21|1555801200|              10.0|\n",
      "|076470|2019| 06| 07|65.0|        24|2019-06-07|1559862000|              24.0|\n",
      "|719200|2019| 10| 09|60.5|        11|2019-10-09|1570575600|              11.0|\n",
      "|949110|2019| 11| 23|54.9|        14|2019-11-23|1574467200|              17.5|\n",
      "|994979|2019| 12| 11|21.3|        21|2019-12-11|1576022400|              17.5|\n",
      "+------+----+---+---+----+----------+----------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the average temperature for a 60 day sliding window\n",
    "# Note that rangeBetween() uses the VALUES rather than ROW NUMBERs.\n",
    "ONE_MONTH_ISH = 30 * 60 * 60 * 24\n",
    "one_month_ish_before_after = (\n",
    "    Window.partitionBy(\"year\")\n",
    "    .orderBy(\"dt_num\")\n",
    "    .rangeBetween(-ONE_MONTH_ISH, ONE_MONTH_ISH)\n",
    ")\n",
    "\n",
    "gsod_light_p.withColumn(\n",
    "    \"avg_count\", F.avg(\"count_temp\").over(one_month_ish_before_after)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10.3\n",
    "\n",
    "If you have a data frame with 1,000,001 rows, where the ordered column ord is defined by `F.lit(10)`, what is the result of the following window functions?\n",
    "\n",
    "1) `F.count(\"ord\").over(Window.partitionBy().orderBy(\"ord\").rowsBetween(-2, 2))`\n",
    "\n",
    "2) `F.count(\"ord\").over(Window.partitionBy().orderBy(\"ord\").rangeBetween(-2, 2))`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|ord|\n",
      "+---+\n",
      "| 10|\n",
      "| 10|\n",
      "| 10|\n",
      "| 10|\n",
      "| 10|\n",
      "| 10|\n",
      "| 10|\n",
      "| 10|\n",
      "| 10|\n",
      "| 10|\n",
      "+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_example = spark.createDataFrame(\n",
    "    [[10] for x in range(1000001)], [\"ord\"] \n",
    ")\n",
    "\n",
    "count_example.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "|ord|row|  range|\n",
      "+---+---+-------+\n",
      "| 10|  3|1000001|\n",
      "| 10|  4|1000001|\n",
      "| 10|  5|1000001|\n",
      "| 10|  5|1000001|\n",
      "| 10|  5|1000001|\n",
      "| 10|  5|1000001|\n",
      "| 10|  5|1000001|\n",
      "| 10|  5|1000001|\n",
      "| 10|  5|1000001|\n",
      "| 10|  5|1000001|\n",
      "+---+---+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_example.select(\n",
    "    \"ord\",\n",
    "    F.count(\"ord\")\n",
    "    .over(Window.partitionBy().orderBy(\"ord\").rowsBetween(-2, 2))\n",
    "    .alias(\"row\"),\n",
    "    F.count(\"ord\")\n",
    "    .over(Window.partitionBy().orderBy(\"ord\").rangeBetween(-2, 2))\n",
    "    .alias(\"range\"),\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
