{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from time import sleep\n",
    "from typing import Iterator, Sequence, Tuple\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we continue to work with the NOAA weather data we worked with in Chapter 9: 10 years' (2010 to 2020) worth of NOAA weather data located in Google BigQuery, which totals over 40 million records. The `bigquery-public-data` is a project available to all.\n",
    "\n",
    "Here, we read in a large amount of data from a warehouse and assemble a single data frame representing weather information across the globe for a period of 10 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Spark connector package version closest to our installed version of Spark (3.4.1)\n",
    "spark = SparkSession.builder.config(\n",
    "    \"spark.jars.packages\",\n",
    "    \"com.google.cloud.spark:spark-3.3-bigquery:0.32.0\"\n",
    ").config(\n",
    "    \"parentProject\", \"cool-wharf-393713\"\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract the table reading routine into a reusable function, returning the resulting data frame\n",
    "def read_df_from_bq(year):\n",
    "    return (\n",
    "        spark.read.format(\"bigquery\").option(\n",
    "            \"table\", f\"bigquery-public-data.noaa_gsod.gsod{year}\"\n",
    "        )\n",
    "        .option(\"credentialsFile\", \"../../../../cool-wharf-393713-73800a184f10.json\")\n",
    "        .load()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsod = (\n",
    "#     reduce(\n",
    "#         # use a lambda function over a list comprehension of data frames to union them all\n",
    "#         lambda x, y: x.unionByName(y, allowMissingColumns=True),\n",
    "#         [read_df_from_bq(year) for year in range(2010, 2021)],\n",
    "#     )\n",
    "#     .dropna(subset=[\"year\", \"mo\", \"da\", \"temp\"])\n",
    "#     .where(F.col(\"temp\") != 9999.9)\n",
    "#     .drop(\"date\")\n",
    "# )\n",
    "\n",
    "# Instead, we work locally with three year's worth of data in Parquet format:\n",
    "gsod = spark.read.parquet(\"../../data/window/gsod.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the coldest day of each year, the long way\n",
    "\n",
    "we want a data frame containing three records, one for each year and showing the station, the date (year, month, day), and the temperature of the coldest day recorded for that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|year|  temp|\n",
      "+----+------+\n",
      "|2017|-114.7|\n",
      "|2019|-114.7|\n",
      "|2018|-113.5|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the lowest temperature for each year using groupBy()\n",
    "coldest_temp = gsod.groupby(\"year\").agg(F.min(\"temp\").alias(\"temp\"))\n",
    "coldest_temp.orderBy(\"temp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+---+------+\n",
      "|   stn|year| mo| da|  temp|\n",
      "+------+----+---+---+------+\n",
      "|896250|2017| 06| 20|-114.7|\n",
      "|896060|2018| 08| 27|-113.5|\n",
      "|895770|2019| 06| 15|-114.7|\n",
      "+------+----+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use a left-semi equi-join on the original table to get \n",
    "# the month and day columns\n",
    "coldest_when = gsod.join(\n",
    "    coldest_temp, how=\"left_semi\", on=[\"year\", \"temp\"]\n",
    ").select(\"stn\", \"year\", \"mo\", \"da\", \"temp\")\n",
    "\n",
    "coldest_when.orderBy(\"year\", \"mo\", \"da\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this instead using a window function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.window.WindowSpec object at 0x0000020E3908EC70>\n"
     ]
    }
   ],
   "source": [
    "# Create a WindowSpec object by using the Window builder class\n",
    "# This forms a blueprint for an eventual window function.\n",
    "each_year = Window.partitionBy(\"year\")\n",
    "\n",
    "print(each_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+------+------+\n",
      "|year| mo| da|   stn|  temp|\n",
      "+----+---+---+------+------+\n",
      "|2017| 06| 20|896250|-114.7|\n",
      "|2018| 08| 27|896060|-113.5|\n",
      "|2019| 06| 15|895770|-114.7|\n",
      "+----+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the minimum temperature for each year using a window function\n",
    "(\n",
    "    gsod\n",
    "    .withColumn(\"min_temp\", F.min(\"temp\").over(each_year))\n",
    "    .where(\"temp = min_temp\")\n",
    "    .select(\"year\", \"mo\", \"da\", \"stn\", \"temp\")\n",
    "    .orderBy(\"year\", \"mo\", \"da\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
