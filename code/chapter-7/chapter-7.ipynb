{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = spark.read.csv(\n",
    "    \"../../data/elements/Periodic_Table_Of_Elements.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|period|count|\n",
      "+------+-----+\n",
      "|     6|    1|\n",
      "|     4|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(elements\n",
    " .where(F.col(\"phase\") == \"liq\")\n",
    " .groupby(\"period\")\n",
    " .count()\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL equivalent for this operation is:\n",
    "```\n",
    "SELECT\n",
    " period,\n",
    " count(*)\n",
    "FROM elements\n",
    "WHERE phase = 'liq'\n",
    "GROUP BY period;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark maintains boundaries between its own name spacing and Spark SQL’s name spacing so we have to explicitly promote them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TABLE_OR_VIEW_NOT_FOUND] The table or view `elements` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
      "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
      "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 29;\n",
      "'Aggregate ['period], ['period, unresolvedalias(count(1), None)]\n",
      "+- 'Filter ('phase = liq)\n",
      "   +- 'UnresolvedRelation [elements], [], false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.sql(\n",
    "        \"select period, count(*) from elements \"\n",
    "        \"where phase='liq' group by period\"\n",
    "    ).show(5)\n",
    "except AnalysisException as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow a data frame to be queried via SQL, we need to _register_ it using `createOrReplaceTempView`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements.createOrReplaceTempView(\"elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|period|count(1)|\n",
      "+------+--------+\n",
      "|     6|       1|\n",
      "|     4|       1|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"select period, count(*) from elements \"\n",
    "    \"where phase='liq' group by period\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the catalog to list the tables/views we have registered and drop them if we are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='elements', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.dropTempView(\"elements\")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backblaze is a company that provides cloud storage and backup. Since 2013, they have provided data on the drives in\n",
    "their data center, and over time have moved to a focus on failures and diagnosis.\n",
    "\n",
    "All data are available from https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data.\n",
    "\n",
    "To avoid blowing our memory, we'll work with just Q3 2019.\n",
    "\n",
    "Since this data is ~3GB when unpacked, it is not checked in to this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"../../data/backblaze/data_Q3_2019\"\n",
    "\n",
    "q3 = spark.read.csv(\n",
    "    DATA_DIRECTORY,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "backblaze_2019 = q3\n",
    "\n",
    "# Set the layour for each column according to the schema\n",
    "\n",
    "backblaze_2019 = backblaze_2019.select(\n",
    "    [\n",
    "        F.col(x).cast(T.LongType()) if x.startswith(\"smart\") else F.col(x)\n",
    "        for x in backblaze_2019.columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "backblaze_2019.createOrReplaceTempView(\"backblaze_stats_2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|serial_number|\n",
      "+-------------+\n",
      "|     ZA10MCJ5|\n",
      "|     ZCH07T9K|\n",
      "|     ZCH0CA7Z|\n",
      "|     Z302F381|\n",
      "|     ZCH0B3Z2|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show hard drive serial numbers that have failed as some point:\n",
    "spark.sql(\n",
    "    \"select serial_number from backblaze_stats_2019 where failure = 1\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|               model|              min_GB| max_GB|\n",
      "+--------------------+--------------------+-------+\n",
      "| TOSHIBA MG07ACA14TA|             13039.0|13039.0|\n",
      "|       ST12000NM0007|-9.31322574615478...|11176.0|\n",
      "|HGST HUH721212ALE600|             11176.0|11176.0|\n",
      "|       ST12000NM0117|             11176.0|11176.0|\n",
      "|HGST HUH721212ALN604|-9.31322574615478...|11176.0|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the capacity in gigabytes of the hardrives in the data\n",
    "spark.sql(\n",
    "    \"\"\"SELECT\n",
    "            model,\n",
    "            min(capacity_bytes / pow(1024, 3)) AS min_GB,\n",
    "            max(capacity_bytes / pow(1024, 3)) AS max_GB\n",
    "        FROM backblaze_stats_2019\n",
    "        GROUP BY 1\n",
    "        ORDER BY 3 DESC\n",
    "        \"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|               model|              min_GB| max_GB|\n",
      "+--------------------+--------------------+-------+\n",
      "| TOSHIBA MG07ACA14TA|             13039.0|13039.0|\n",
      "|       ST12000NM0007|-9.31322574615478...|11176.0|\n",
      "|HGST HUH721212ALE600|             11176.0|11176.0|\n",
      "|       ST12000NM0117|             11176.0|11176.0|\n",
      "|HGST HUH721212ALN604|-9.31322574615478...|11176.0|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the capacity in gigabytes of the hardrives in the data\n",
    "backblaze_2019.groupby(F.col(\"model\")).agg(\n",
    "    F.min(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"min_GB\"),\n",
    "    F.max(F.col(\"capacity_bytes\") / F.pow(F.lit(1024), 3)).alias(\"max_GB\")\n",
    ").orderBy(F.col(\"max_GB\"), ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results from our query, there are some drives that report more than one capacity. Furthermore, we have some drives that report negative capacity, which is really odd. Let’s focus on seeing how prevalent this is.\n",
    "\n",
    "We assume that the maximum reported capacity for each model is the correct one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+\n",
      "|               model|              min_GB|           max_GB|\n",
      "+--------------------+--------------------+-----------------+\n",
      "|       ST12000NM0007|-9.31322574615478...|          11176.0|\n",
      "|HGST HUH721212ALN604|-9.31322574615478...|          11176.0|\n",
      "|HGST HUH721010ALE600|-9.31322574615478...|           9314.0|\n",
      "|       ST10000NM0086|-9.31322574615478...|           9314.0|\n",
      "|        ST8000NM0055|-9.31322574615478...|7452.036460876465|\n",
      "+--------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many models have more than one capacity?\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        model,\n",
    "        min(capacity_bytes / pow(1024, 3)) AS min_GB,\n",
    "        max(capacity_bytes / pow(1024, 3)) AS max_GB\n",
    "    FROM backblaze_stats_2019\n",
    "    GROUP BY 1\n",
    "    HAVING min_GB != max_GB\n",
    "    ORDER BY 3 DESC\n",
    "\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "backblaze_2019.createOrReplaceTempView(\"drive_stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW drive_days AS\n",
    "        SELECT model, count(*) AS drive_days\n",
    "        FROM drive_stats\n",
    "        GROUP BY model\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW failures AS\n",
    "        SELECT model, count(*) AS failures\n",
    "        FROM drive_stats\n",
    "        WHERE failure = 1\n",
    "        GROUP BY model\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_days = (\n",
    "    backblaze_2019.groupby(F.col(\"model\"))\n",
    "    .agg(F.count(F.col(\"*\")).alias(\"drive_days\"))\n",
    ")\n",
    "\n",
    "failures = (\n",
    "    backblaze_2019.where(F.col(\"failure\") == 1)\n",
    "    .groupby(F.col(\"model\"))\n",
    "    .agg(F.count(F.col(\"*\")).alias(\"failures\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------+\n",
      "|        model|drive_days|failures|\n",
      "+-------------+----------+--------+\n",
      "|  ST9250315AS|        89|    null|\n",
      "|  ST4000DM000|   1796728|      72|\n",
      "|ST12000NM0007|   3212635|     361|\n",
      "|  ST8000DM005|      2280|       1|\n",
      "|   ST320LT007|        89|    null|\n",
      "+-------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT drive_days.model, drive_days, failures\n",
    "    FROM drive_days\n",
    "    LEFT JOIN failures\n",
    "    ON drive_days.model = failures.model\n",
    "    \"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|             model|        failure_rate|\n",
      "+------------------+--------------------+\n",
      "|     ST12000NM0117|0.019305019305019305|\n",
      "|TOSHIBA MQ01ABF050|5.579360828423496E-4|\n",
      "|       ST8000DM005|4.385964912280702E-4|\n",
      "|        ST500LM030| 4.19639110365086E-4|\n",
      "|     ST500LM012 HN|1.511585221015353...|\n",
      "+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SUBQUERIES\n",
    "# Find the models with the highest failure rates in Q3 2019\n",
    "# by querying drive_stats directly, using subqueries\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT failures.model, failures / drive_days AS failure_rate\n",
    "    FROM (\n",
    "        SELECT model, count(*) AS drive_days\n",
    "        FROM drive_stats\n",
    "        GROUP BY model\n",
    "    ) AS drive_days\n",
    "    INNER JOIN (\n",
    "        SELECT model, count(*) AS failures\n",
    "        FROM drive_stats\n",
    "        WHERE failure = 1\n",
    "        GROUP BY model\n",
    "    ) AS failures\n",
    "    ON drive_days.model = failures.model\n",
    "    ORDER BY 2 desc\n",
    "    \"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|             model|        failure_rate|\n",
      "+------------------+--------------------+\n",
      "|     ST12000NM0117|0.019305019305019305|\n",
      "|TOSHIBA MQ01ABF050|5.579360828423496E-4|\n",
      "|       ST8000DM005|4.385964912280702E-4|\n",
      "|        ST500LM030| 4.19639110365086E-4|\n",
      "|     ST500LM012 HN|1.511585221015353...|\n",
      "+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CTEs\n",
    "# Find the models with the highest failure rates in Q3 2019\n",
    "# by querying drive_stats directly, using subqueries CTEs\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    WITH drive_days AS (\n",
    "        SELECT model, count(*) AS drive_days\n",
    "        FROM drive_stats\n",
    "        GROUP BY model\n",
    "    ),\n",
    "    failures AS (\n",
    "        SELECT model, count(*) AS failures\n",
    "        FROM drive_stats\n",
    "        WHERE failure = 1\n",
    "        GROUP BY model\n",
    "    )\n",
    "    SELECT failures.model, failures / drive_days AS failure_rate\n",
    "    FROM drive_days \n",
    "    INNER JOIN failures\n",
    "    ON drive_days.model = failures.model\n",
    "    ORDER BY 2 desc\n",
    "    \"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python function\n",
    "# Find the models with the highest failure rates in Q3 2019\n",
    "# by querying drive_stats directly\n",
    "\n",
    "def failure_rate(drive_stats):\n",
    "    drive_days = drive_stats.groupby(F.col(\"model\")).agg(\n",
    "        F.count(F.col(\"*\")).alias(\"drive_days\")\n",
    "    )\n",
    "\n",
    "    failures = (\n",
    "        drive_stats.where(F.col(\"failure\") == 1)\n",
    "        .groupby(F.col(\"model\"))\n",
    "        .agg(F.count(F.col(\"*\")).alias(\"failures\"))\n",
    "    )\n",
    "\n",
    "    answer = (\n",
    "        drive_days.join(failures, on=\"model\", how=\"inner\")\n",
    "        .withColumn(\"failure_rate\", F.col(\"failures\") / F.col(\"drive_days\"))\n",
    "        .orderBy(F.col(\"failure_rate\").desc())\n",
    "    )\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+--------+--------------------+\n",
      "|             model|drive_days|failures|        failure_rate|\n",
      "+------------------+----------+--------+--------------------+\n",
      "|     ST12000NM0117|       259|       5|0.019305019305019305|\n",
      "|TOSHIBA MQ01ABF050|     44808|      25|5.579360828423496E-4|\n",
      "|       ST8000DM005|      2280|       1|4.385964912280702E-4|\n",
      "|        ST500LM030|     21447|       9| 4.19639110365086E-4|\n",
      "|     ST500LM012 HN|     46309|       7|1.511585221015353...|\n",
      "+------------------+----------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "failure_rate(backblaze_2019).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.1\n",
    "\n",
    "Taking the elements data frame, which PySpark code is equivalent to the following SQL statement?\n",
    "\n",
    "```\n",
    "select count(*) from elements where Radioactive is not null;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements.where(F.col(\"Radioactive\").isNotNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SQL-style expressions in PySpark\n",
    "\n",
    "We can take an alternative approach where we pre-process our data so that it more easily answers our question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = backblaze_2019.selectExpr(\n",
    "    \"model\", \"capacity_bytes / pow(1024, 3) AS capacity_GB\", \"date\", \"failure\"\n",
    ")\n",
    "\n",
    "drive_days = full_data.groupby(\"model\", \"capacity_GB\").agg(\n",
    "    F.count(\"*\").alias(\"drive_days\")\n",
    ")\n",
    "\n",
    "failures = (\n",
    "    full_data.where(\"failure = 1\")\n",
    "    .groupby(\"model\", \"capacity_GB\")\n",
    "    .agg(F.count(\"*\").alias(\"failures\"))\n",
    ")\n",
    "\n",
    "summarised_data = (\n",
    "    drive_days.join(failures, on=[\"model\", \"capacity_GB\"], how=\"left\")\n",
    "    .fillna(0.0, [\"failures\"])\n",
    "    .selectExpr(\"model\", \"capacity_GB\", \"failures / drive_days AS failure_rate\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_reliable_drive_for_capacity(data, \n",
    "                                     capacity_GB=2048, \n",
    "                                     precision=0.25,\n",
    "                                     top_n=3):\n",
    "    capacity_min = capacity_GB / (1 + precision)\n",
    "    capacity_max = capacity_GB * (1 + precision)\n",
    "\n",
    "    answer = (\n",
    "        data.where(f\"capacity_GB between {capacity_min} and {capacity_max}\")\n",
    "        .orderBy(\"failure_rate\", \"capacity_GB\", ascending=[True, True])\n",
    "        .limit(top_n)\n",
    "    )\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+\n",
      "|               model|capacity_GB|        failure_rate|\n",
      "+--------------------+-----------+--------------------+\n",
      "|HGST HUH721010ALE600|     9314.0|                 0.0|\n",
      "|HGST HUH721212ALN604|    11176.0|1.585588480593982...|\n",
      "|HGST HUH721212ALE600|    11176.0|1.636661211129296...|\n",
      "+--------------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_reliable_drive_for_capacity(summarised_data, capacity_GB=11176.0).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
