{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"../../data/backblaze/data_Q3_2019\"\n",
    "\n",
    "q3 = spark.read.csv(\n",
    "    DATA_DIRECTORY,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "backblaze_2019 = q3\n",
    "\n",
    "# Set the layour for each column according to the schema\n",
    "\n",
    "backblaze_2019 = backblaze_2019.select(\n",
    "    [\n",
    "        F.col(x).cast(T.LongType()) if x.startswith(\"smart\") else F.col(x)\n",
    "        for x in backblaze_2019.columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "backblaze_2019.createOrReplaceTempView(\"backblaze_stats_2019\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2\n",
    "\n",
    "If we look at the code that follows, we can simplify it even further and avoid creating two tables outright. Can you write a summarized_data without having to use a table\n",
    "other than full_data and no join? (Bonus: Try using pure PySpark, then pure Spark\n",
    "SQL, and then a combo of both.)\n",
    "\n",
    "```\n",
    "full_data = backblaze2019.selectExpr(\n",
    " \"model\", \"capacity_bytes / pow(1024, 3) capacity_GB\", \"date\", \"failure\"\n",
    ")\n",
    "\n",
    "drive_days = full_data.groupby(\"model\", \"capacity_GB\").agg(\n",
    " F.count(\"*\").alias(\"drive_days\")\n",
    ")\n",
    "\n",
    "failures = (\n",
    " full_data.where(\"failure = 1\")\n",
    " .groupby(\"model\", \"capacity_GB\")\n",
    " .agg(F.count(\"*\").alias(\"failures\"))\n",
    ")\n",
    "\n",
    "summarized_data = (\n",
    " drive_days.join(failures, on=[\"model\", \"capacity_GB\"], how=\"left\")\n",
    " .fillna(0.0, [\"failures\"])\n",
    " .selectExpr(\"model\", \"capacity_GB\", \"failures / drive_days failure_rate\")\n",
    " .cache()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|               model|         capacity_GB|        failure_rate|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|       ST12000NM0117|             11176.0|0.019305019305019305|\n",
      "|      WDC WD5000LPCX|   465.7617416381836|                 0.0|\n",
      "|         ST6000DM004|    5589.02986907959|                 0.0|\n",
      "|         ST4000DM005|   3726.023277282715|                 0.0|\n",
      "|HGST HMS5C4040BLE641|   3726.023277282715|                 0.0|\n",
      "|       ST500LM012 HN|   465.7617416381836|1.511585221015353...|\n",
      "|HGST HUH721010ALE600|-9.31322574615478...|                 0.0|\n",
      "|         ST6000DX000|    5589.02986907959|4.907252919815487...|\n",
      "|       ST12000NM0007|-9.31322574615478...|                 0.0|\n",
      "|         ST8000DM004|   7452.036460876465|                 0.0|\n",
      "|         ST6000DM001|    5589.02986907959|                 0.0|\n",
      "|      WDC WD5000BPKT|   465.7617416381836|                 0.0|\n",
      "|        WDC WD60EFRX|    5589.02986907959|                 0.0|\n",
      "|HGST HMS5C4040ALE640|-9.31322574615478...|                 0.0|\n",
      "|Seagate BarraCuda...|   465.7617416381836|                 0.0|\n",
      "|       ST12000NM0007|             11176.0|1.123919783883613E-4|\n",
      "|HGST HUH721010ALE600|              9314.0|                 0.0|\n",
      "| TOSHIBA MQ01ABF050M|   465.7617416381836|1.414387146049616...|\n",
      "|         ST8000DM002|-9.31322574615478...|                 0.0|\n",
      "|        ST8000NM0055|-9.31322574615478...|0.003663003663003663|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pure PySpark version\n",
    "\n",
    "full_data = backblaze_2019.selectExpr(\n",
    " \"model\", \"capacity_bytes / pow(1024, 3) AS capacity_GB\", \"date\", \"failure\"\n",
    ")\n",
    "\n",
    "# Calculate drive_days\n",
    "drive_days = full_data.groupBy(\"model\", \"capacity_GB\").agg(\n",
    "    F.count(\"*\").alias(\"drive_days\")\n",
    ")\n",
    "\n",
    "# Calculate failures\n",
    "failures = full_data.where(\"failure = 1\").groupBy(\"model\", \"capacity_GB\").agg(\n",
    "    F.count(\"*\").alias(\"failures\")\n",
    ")\n",
    "\n",
    "# Join and calculate failure_rate\n",
    "summarized_data = drive_days.join(failures, on=[\"model\", \"capacity_GB\"], how=\"left\")\\\n",
    "    .fillna(0.0, [\"failures\"])\\\n",
    "    .withColumn(\"failure_rate\", F.col(\"failures\") / F.col(\"drive_days\"))\\\n",
    "    .select(\"model\", \"capacity_GB\", \"failure_rate\")\\\n",
    "    .cache()\n",
    "\n",
    "# Show the summarized data\n",
    "summarized_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+\n",
      "|             model|       capacity_GB|        failure_rate|\n",
      "+------------------+------------------+--------------------+\n",
      "|       ST9250315AS|232.88591766357422|                 0.0|\n",
      "|        ST320LT007|298.09114837646484|                 0.0|\n",
      "|    WDC WD5000LPVX| 465.7617416381836|5.070222582771384E-5|\n",
      "|TOSHIBA MQ01ABF050| 465.7617416381836|5.579360828423496E-4|\n",
      "|   TOSHIBA HDWE160|  5589.02986907959|                 0.0|\n",
      "+------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pure SQL version\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    -- Calculate drive_days\n",
    "    WITH drive_days AS (\n",
    "    SELECT\n",
    "        model,\n",
    "        capacity_bytes / pow(1024, 3) AS capacity_GB,\n",
    "        COUNT(*) AS drive_days\n",
    "    FROM\n",
    "        backblaze_stats_2019\n",
    "    GROUP BY\n",
    "        model,\n",
    "        capacity_bytes\n",
    "    ),\n",
    "\n",
    "    -- Calculate failures\n",
    "    failures AS (\n",
    "    SELECT\n",
    "        model,\n",
    "        capacity_bytes / pow(1024, 3) AS capacity_GB,\n",
    "        COUNT(*) AS failures\n",
    "    FROM\n",
    "        backblaze_stats_2019\n",
    "    WHERE\n",
    "        failure = 1\n",
    "    GROUP BY\n",
    "        model,\n",
    "        capacity_bytes\n",
    "    )\n",
    "\n",
    "    -- Calculate summarized_data\n",
    "    SELECT\n",
    "    dd.model,\n",
    "    dd.capacity_GB,\n",
    "    COALESCE(f.failures, 0) / dd.drive_days AS failure_rate\n",
    "    FROM\n",
    "    drive_days dd\n",
    "    LEFT JOIN\n",
    "    failures f ON dd.model = f.model AND dd.capacity_GB = f.capacity_GB;\n",
    "    \"\"\" \n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.3\n",
    "\n",
    "The analysis in the chapter is flawed in that the age of a drive is not taken into consideration. Instead of ordering the model by failure rate, order by average age at failure\n",
    "(assume that every drive fails on the maximum date reported if they are still alive).\n",
    "(Hint: Remember that you need to count the age of each drive first.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.4\n",
    "\n",
    "What is the total capacity (in TB) that Backblaze records at the beginning of each month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|      date|capacity_TB|\n",
      "+----------+-----------+\n",
      "|2019-07-01|   833798.0|\n",
      "|2019-08-01|   846104.0|\n",
      "|2019-09-01|   858427.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT date, round(sum(capacity_bytes) / pow(1024, 4), 0) AS capacity_TB\n",
    "    FROM backblaze_stats_2019\n",
    "    WHERE failure = 0\n",
    "    AND date = DATE_FORMAT(date, 'yyyy-MM-01')\n",
    "    GROUP BY date\n",
    "    ORDER BY date\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.5\n",
    "\n",
    "If you look at the data, youâ€™ll see that some drive models can report an erroneous\n",
    "capacity. In the data preparation stage, restage the full_data data frame so that the\n",
    "most common capacity for each drive is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can update the data preparation stage like this:\n",
    "\n",
    "full_data = backblaze_2019.selectExpr(\n",
    " \"model\", \"capacity_bytes / pow(1024, 3) AS capacity_GB\", \"date\", \"failure\"\n",
    ")\n",
    "\n",
    "# Calculate the mode capacity_GB for each model\n",
    "mode_capacity_gb = backblaze_2019.groupby(\"model\").agg(\n",
    "    F.expr(\"percentile_approx(capacity_bytes / pow(1024, 3), 0.5)\").alias(\"mode_capacity_GB\")\n",
    ")\n",
    "\n",
    "# Join the mode_capacity_gb with the full_data DataFrame\n",
    "full_data = backblaze_2019.join(mode_capacity_gb, on=\"model\", how=\"inner\")\n",
    "\n",
    "# Create a new column \"capacity_GB\" using the mode_capacity_GB\n",
    "full_data = full_data.withColumn(\"capacity_GB\", F.col(\"mode_capacity_GB\"))\n",
    "\n",
    "# Select the required columns and continue with your existing query\n",
    "full_data = full_data.selectExpr(\"model\", \"capacity_GB\", \"date\", \"failure\")\n",
    "\n",
    "# Show the updated DataFrame\n",
    "# full_data.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
