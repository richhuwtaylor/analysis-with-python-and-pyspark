{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3\n",
    "\n",
    "Reread the data in a `logs_raw` data frame (the data file is `./data/broadcast_logsBroadcastLogs_2018_Q3_M8.CSV`), this time without passing any optional parameters.\n",
    "Print the first five rows of data, as well as the schema. What are the differences in terms of data and schema between logs and logs_raw?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the broadcasting information\n",
    "DIRECTORY = \"../../data/broadcast_logs\"\n",
    "logs_raw = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8_sample.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|BroadcastLogID|Lo...|\n",
      "|1196192316|3157|2...|\n",
      "|1196192317|3157|2...|\n",
      "|1196192318|3157|2...|\n",
      "|1196192319|3157|2...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_raw.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`logs_raw` hasn't been split into separate solumns and instead consists of a single string column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.4\n",
    "Create a new data frame, logs_clean, that contains only the columns that do not end with ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_clean = spark.read.csv(\n",
    "    os.path.join(DIRECTORY, \"BroadcastLogs_2018_Q3_M8_sample.csv\"),\n",
    "    sep=\"|\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    timestampFormat=\"yyyy-MM-dd\"\n",
    ")\n",
    "\n",
    "logs_clean = logs_clean.select([col for col in logs_clean.columns if not col.endswith(\"ID\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LogDate',\n",
       " 'SequenceNO',\n",
       " 'Duration',\n",
       " 'EndTime',\n",
       " 'LogEntryDate',\n",
       " 'ProductionNO',\n",
       " 'ProgramTitle',\n",
       " 'StartTime',\n",
       " 'Subtitle',\n",
       " 'Producer1',\n",
       " 'Producer2',\n",
       " 'Language1',\n",
       " 'Language2']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_clean.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
