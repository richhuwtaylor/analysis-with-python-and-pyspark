{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\n",
    "    \"Counting word occurences from a book.\"\n",
    ").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3\n",
    "\n",
    "1) By modifying the `word_count_submit.py` program, return the number of distinct words in Jane Austen’s Pride and Prejudice. (Hint: results contains one\n",
    "record for each unique word.)\n",
    "\n",
    "2) (Challenge) Wrap your program in a function that takes a file name as a parameter. It should return the number of distinct words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to read multiple text files, replace `1342-0` by `*`.\n",
    "results = (\n",
    "    spark.read.text(\"../../data/gutenberg_books/1342-0.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .agg(F.countDistinct(F.col(\"word\")).alias(\"count\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "| 6595|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.4\n",
    "\n",
    "Taking `word_count_submit.py`, modify the script to return a sample of five words that\n",
    "appear only once in Jane Austen’s Pride and Prejudice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = (\n",
    "    spark.read.text(\"../../data/gutenberg_books/1342-0.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .groupby(F.col(\"word\"))\n",
    "    .count()\n",
    "    .where(F.col(\"count\") == 1)\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|   imitation|    1|\n",
      "|     solaced|    1|\n",
      "|premeditated|    1|\n",
      "|     elevate|    1|\n",
      "|   destitute|    1|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.5\n",
    "\n",
    "1) Using the substring function (refer to PySpark’s API or the pyspark shell if\n",
    "needed), return the top five most popular first letters (keep only the first letter\n",
    "of each word).\n",
    "\n",
    "2) Compute the number of words starting with a consonant or a vowel. (Hint: The\n",
    "isin() function might be useful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = (\n",
    "    spark.read.text(\"../../data/gutenberg_books/1342-0.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .groupby(F.substring(F.col(\"word\"), 1, 1).alias('letter'))\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|letter|count|\n",
      "+------+-----+\n",
      "|     t|16101|\n",
      "|     a|13684|\n",
      "|     h|10419|\n",
      "|     w| 9091|\n",
      "|     s| 8791|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = (\n",
    "    spark.read.text(\"../../data/gutenberg_books/1342-0.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .select(F.substring(F.col(\"word\"), 1, 1).alias('letter'))\n",
    "    .select(F.when(F.col(\"letter\").isin(\"a\", \"e\", \"i\", \"o\", \"u\"), \"vowel\").otherwise(\"consonant\").alias(\"category\"))\n",
    "    .groupby(F.col(\"category\"))\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "| category|count|\n",
      "+---------+-----+\n",
      "|consonant|88653|\n",
      "|    vowel|33522|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.6\n",
    "\n",
    "Let’s say you want to get both the count() and sum() of a GroupedData object. Why\n",
    "doesn’t this code work? Map the inputs and outputs of each method.\n",
    "\n",
    "`my_data_frame.groupby(\"my_column\").count().sum()`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code my_data_frame.groupby(\"my_column\").count().sum() doesn't work because the sum() method is not applicable to a GroupedData object in PySpark. The sum() method is used for aggregating numerical columns and not for aggregating the counts.\n",
    "\n",
    "To get both the count and sum of a column in a DataFrame, you need to perform separate aggregation operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (my_data_frame.groupby(\"my_column\")\n",
    "          .agg(F.count(\"my_column\").alias(\"count\"), F.sum(\"my_column\").alias(\"sum\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
